{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# change path to parent directory for paths\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import accelerate\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from diffusers.optimization import get_scheduler\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "from ldns.networks import AutoEncoder, CountWrapper\n",
    "from ldns.utils.plotting_utils import *\n",
    "from ldns.losses import latent_regularizer\n",
    "from ldns.networks import Denoiser\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.schedulers import DDPMScheduler\n",
    "\n",
    "import lovely_tensors as lt\n",
    "\n",
    "lt.monkey_patch()\n",
    "\n",
    "matplotlib.rc_file(\"matplotlibrc\")\n",
    "import warnings\n",
    "\n",
    "# suppress FutureWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "# suppress all font manager warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib.font_manager\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"findfont: Generic family\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config and model path\n",
    "\n",
    "cfg_ae = OmegaConf.load(\"conf/autoencoder-human.yaml\")\n",
    "cfg = OmegaConf.load(\"conf/diffusion_human.yaml\")\n",
    "\n",
    "cfg.dataset = cfg_ae.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldns.data.human import get_human_dataloaders\n",
    "\n",
    "# set seed\n",
    "torch.manual_seed(cfg.training.random_seed)\n",
    "np.random.seed(cfg.training.random_seed)\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = get_human_dataloaders(\n",
    "    cfg_ae.dataset.datapath,\n",
    "    batch_size=cfg_ae.training.batch_size,\n",
    "    shuffle_train=False,  # for eval\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model = AutoEncoder(\n",
    "    C_in=cfg_ae.model.C_in,\n",
    "    C=cfg_ae.model.C,\n",
    "    C_latent=cfg_ae.model.C_latent,\n",
    "    L=cfg_ae.dataset.max_seqlen,\n",
    "    num_blocks=cfg_ae.model.num_blocks,\n",
    "    num_blocks_decoder=cfg_ae.model.get(\"num_blocks_decoder\", cfg_ae.model.num_blocks),\n",
    "    num_lin_per_mlp=cfg_ae.model.get(\"num_lin_per_mlp\", 2),  # default 2\n",
    "    bidirectional=cfg_ae.model.get(\"bidirectional\", False),\n",
    ")\n",
    "\n",
    "ae_model = CountWrapper(ae_model)\n",
    "ae_model.load_state_dict(torch.load(f\"exp/stored_models/{cfg_ae.exp_name}/model.pt\"))\n",
    "\n",
    "accelerator = accelerate.Accelerator(mixed_precision=\"no\")\n",
    "\n",
    "ae_model = accelerator.prepare(ae_model)\n",
    "print(cfg_ae.exp_name)\n",
    "\n",
    "(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    test_dataloader,\n",
    ") = accelerator.prepare(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    test_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldns.data.human import LatentHumanDataset\n",
    "\n",
    "# create training dataset\n",
    "latent_dataset_train = LatentHumanDataset(train_dataloader, ae_model, clip=False)\n",
    "\n",
    "# create validation and test datasets using training set statistics\n",
    "latent_dataset_val = LatentHumanDataset(\n",
    "    val_dataloader,\n",
    "    ae_model,\n",
    "    latent_means=latent_dataset_train.latent_means,\n",
    "    latent_stds=latent_dataset_train.latent_stds,\n",
    "    clip=False,\n",
    ")\n",
    "latent_dataset_test = LatentHumanDataset(\n",
    "    test_dataloader,\n",
    "    ae_model,\n",
    "    latent_means=latent_dataset_train.latent_means,\n",
    "    latent_stds=latent_dataset_train.latent_stds,\n",
    "    clip=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(latent_dataset_train[0])\n",
    "display(latent_dataset_train[1])\n",
    "\n",
    "element = latent_dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(element[\"latent\"][1])\n",
    "plt.plot(element[\"mask\"][0])\n",
    "\n",
    "# mask and (padded) latents visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_latent_dataloader = torch.utils.data.DataLoader(\n",
    "    latent_dataset_train,\n",
    "    batch_size=cfg.training.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_latent_dataloader = torch.utils.data.DataLoader(\n",
    "    latent_dataset_val,\n",
    "    batch_size=cfg.training.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "num_batches = len(train_latent_dataloader)\n",
    "\n",
    "# check if signal length is power of 2\n",
    "if cfg.dataset.max_seqlen & (cfg.dataset.max_seqlen - 1) != 0:\n",
    "    cfg.training.precision = \"no\"  # torch.fft doesnt support half if L!=2^x\n",
    "\n",
    "# prepare the denoiser model and dataset\n",
    "(\n",
    "    train_latent_dataloader,\n",
    "    val_latent_dataloader,\n",
    ") = accelerator.prepare(\n",
    "    train_latent_dataloader,\n",
    "    val_latent_dataloader,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize denoiser\n",
    "\n",
    "denoiser = Denoiser(\n",
    "    C_in=cfg.denoiser_model.C_in + 1,  # 1 for mask (length of required latent)\n",
    "    C=cfg.denoiser_model.C,\n",
    "    L=cfg.dataset.max_seqlen,\n",
    "    num_blocks=cfg.denoiser_model.num_blocks,\n",
    "    bidirectional=cfg.denoiser_model.get(\"bidirectional\", True),\n",
    ")\n",
    "\n",
    "denoiser.load_state_dict(torch.load(f\"exp/stored_models/{cfg.exp_name}/model.pt\"))  # load after training\n",
    "\n",
    "scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=cfg.denoiser_model.num_train_timesteps,\n",
    "    clip_sample=False,\n",
    "    beta_schedule=\"linear\",\n",
    ")\n",
    "\n",
    "# prepare the denoiser model\n",
    "denoiser = accelerator.prepare(denoiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_spikes_with_mask(denoiser, scheduler, ae, cfg, lengths=None, batch_size=1, device=\"cuda\"):\n",
    "    \"\"\"Sample spike trains from the diffusion model with variable length masks.\n",
    "\n",
    "    Uses DDPM sampling with EMA-averaged model to generate latents, which are then\n",
    "    decoded to rates and spikes.\n",
    "    Args:\n",
    "        denoiser: denoiser model\n",
    "        scheduler: DDPM scheduler for sampling\n",
    "        ae: Trained autoencoder for decoding latents to rates\n",
    "        cfg: Config object with model parameters\n",
    "        lengths: Optional list/tensor of sequence lengths for masks\n",
    "        batch_size: Number of samples to generate\n",
    "        device: Device to run sampling on\n",
    "\n",
    "    Returns:\n",
    "        dict containing:\n",
    "            rates: Generated firing rates\n",
    "            spikes: Sampled spike trains\n",
    "            latents: Generated latents\n",
    "            masks: Generated masks\n",
    "            mask_lengths: Lengths used for masks\n",
    "    \"\"\"\n",
    "    # start with random noise matching model input shape\n",
    "    z_t = torch.randn((batch_size, cfg.denoiser_model.C_in, cfg.dataset.max_seqlen)).to(device)\n",
    "\n",
    "    # generate lengths for masks if not provided\n",
    "    if lengths is None:\n",
    "        lengths = torch.linspace(100, 512, batch_size).long().to(device)\n",
    "    else:\n",
    "        if isinstance(lengths, int):\n",
    "            lengths = torch.tensor([lengths] * batch_size).to(device)\n",
    "        elif isinstance(lengths, list):\n",
    "            lengths = torch.tensor(lengths).long().to(device)\n",
    "\n",
    "    # create masks with 1s in center and 0s for padding\n",
    "    masks = torch.zeros(batch_size, cfg.dataset.max_seqlen).to(device)\n",
    "    for i, l in enumerate(lengths):\n",
    "        padding_left = (cfg.dataset.max_seqlen - l) // 2\n",
    "        masks[i, padding_left : padding_left + l] = 1.0\n",
    "\n",
    "    masks = masks.unsqueeze(1)\n",
    "\n",
    "    # get EMA model and prepare for inference\n",
    "    denoiser.eval()\n",
    "    scheduler.set_timesteps(cfg.denoiser_model.num_train_timesteps)\n",
    "\n",
    "    # iteratively denoise using DDPM\n",
    "    for t in tqdm(scheduler.timesteps, desc=\"Sampling DDPM (different masks)\"):\n",
    "        with torch.no_grad():\n",
    "            model_output = denoiser(torch.cat([z_t, masks], dim=1), torch.tensor([t] * batch_size).to(device).long())[\n",
    "                :, :-1\n",
    "            ]\n",
    "        z_t = scheduler.step(model_output, t, z_t, return_dict=False)[0]\n",
    "\n",
    "    # scale latents back to original range\n",
    "    z_t = z_t * latent_dataset_train.latent_stds.to(z_t.device) + latent_dataset_train.latent_means.to(z_t.device)\n",
    "\n",
    "    # decode latents to rates and sample spikes\n",
    "    with torch.no_grad():\n",
    "        rates = ae.decode(z_t).cpu()\n",
    "\n",
    "    spikes = torch.poisson(rates)\n",
    "\n",
    "    return {\n",
    "        \"rates\": rates,\n",
    "        \"spikes\": spikes,\n",
    "        \"latents\": z_t.cpu(),\n",
    "        \"masks\": masks.cpu(),\n",
    "        \"mask_lengths\": lengths,\n",
    "    }\n",
    "\n",
    "\n",
    "def reconstruct_spikes(model, dataloader):\n",
    "    \"\"\"Reconstruct spikes from a trained autoencoder model.\n",
    "\n",
    "    Args:\n",
    "        model: Trained autoencoder model\n",
    "        dataloader: DataLoader containing batches of spike data\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing:\n",
    "            - latents: Encoded latent vectors (batch_size, latent_dim)\n",
    "            - spikes: Original spike trains (batch_size, n_channels, seq_len)\n",
    "            - rec_spikes: Reconstructed spike trains (batch_size, n_channels, seq_len)\n",
    "            - signal_masks: Masks indicating valid timesteps (batch_size, 1, seq_len)\n",
    "    \"\"\"\n",
    "    # set model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # initialize lists to store outputs\n",
    "    latents = []\n",
    "    spikes = []\n",
    "    rec_spikes = []\n",
    "    signal_masks = []\n",
    "\n",
    "    # process each batch\n",
    "    for batch in dataloader:\n",
    "        signal = batch[\"signal\"]\n",
    "        signal_mask = batch[\"mask\"]\n",
    "\n",
    "        # get model outputs without gradients\n",
    "        with torch.no_grad():\n",
    "            output_rates, z = model(signal)\n",
    "            z = z.cpu()\n",
    "\n",
    "        # store outputs\n",
    "        latents.append(z)\n",
    "        spikes.append(signal.cpu())\n",
    "        rec_spikes.append(torch.poisson(output_rates.cpu()) * signal_mask.cpu())\n",
    "        signal_masks.append(signal_mask.cpu())\n",
    "\n",
    "    # concatenate all batches\n",
    "    return {\n",
    "        \"latents\": torch.cat(latents, 0),\n",
    "        \"spikes\": torch.cat(spikes, 0),\n",
    "        \"rec_spikes\": torch.cat(rec_spikes, 0),\n",
    "        \"signal_masks\": torch.cat(signal_masks, 0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_real_vs_sampled_rates_and_spikes(\n",
    "    real_rates,\n",
    "    sampled_rates,\n",
    "    real_spikes,\n",
    "    sampled_spikes,\n",
    "    real_masks,\n",
    "    sampled_masks,\n",
    "    batch_idx=0,\n",
    "):\n",
    "    \"\"\"Plot real and sampled rates and spikes side by side for comparison.\n",
    "\n",
    "    Args:\n",
    "        real_rates (torch.Tensor): Ground truth firing rates [batch, channels, length]\n",
    "        sampled_rates (torch.Tensor): Model generated firing rates [batch, channels, length]\n",
    "        real_spikes (torch.Tensor): Ground truth spike trains [batch, channels, length]\n",
    "        sampled_spikes (torch.Tensor): Model generated spike trains [batch, channels, length]\n",
    "        real_masks (torch.Tensor): Masks for real data [batch, 1, length]\n",
    "        sampled_masks (torch.Tensor): Masks for sampled data [batch, 1, length]\n",
    "        batch_idx (int): Which batch element to plot. Defaults to 0.\n",
    "    \"\"\"\n",
    "    B, C, L = real_rates.shape\n",
    "\n",
    "    # create 2x2 subplot with appropriate size\n",
    "    fig, axs = plt.subplots(2, 2, figsize=cm2inch(12, 8), dpi=300)\n",
    "\n",
    "    # select single batch element to plot\n",
    "    real_rates = real_rates[batch_idx]\n",
    "    sampled_rates = sampled_rates[batch_idx]\n",
    "    real_spikes = real_spikes[batch_idx]\n",
    "    sampled_spikes = sampled_spikes[batch_idx]\n",
    "    real_masks = real_masks[batch_idx]\n",
    "    sampled_masks = sampled_masks[batch_idx]\n",
    "\n",
    "    # get indices where mask is 1 (non-padding)\n",
    "    real_mask_idx_with_1 = torch.arange(real_masks[0].nonzero().flatten().numel())\n",
    "    sampled_mask_idx_with_1 = sampled_masks[0].nonzero().flatten()\n",
    "\n",
    "    # plot real rates with colorbar\n",
    "    im = axs[0, 0].imshow(real_rates[:, real_mask_idx_with_1], cmap=\"viridis\", alpha=1.0, aspect=\"auto\")\n",
    "    axs[0, 0].set_title(\"Real rates\")\n",
    "    fig.colorbar(im, ax=axs[0, 0], orientation=\"vertical\", fraction=0.046, pad=0.04)\n",
    "\n",
    "    # plot sampled rates with colorbar\n",
    "    im = axs[0, 1].imshow(sampled_rates[:, sampled_mask_idx_with_1], cmap=\"viridis\", alpha=1.0, aspect=\"auto\")\n",
    "    axs[0, 1].set_title(\"Sampled rates\")\n",
    "    fig.colorbar(im, ax=axs[0, 1], orientation=\"vertical\", fraction=0.046, pad=0.04)\n",
    "\n",
    "    # plot real spikes with colorbar\n",
    "    im = axs[1, 0].imshow(real_spikes[:, real_mask_idx_with_1], cmap=\"Greys\", alpha=1.0, aspect=\"auto\")\n",
    "    axs[1, 0].set_title(\"Real spikes\")\n",
    "    fig.colorbar(im, ax=axs[1, 0], orientation=\"vertical\", fraction=0.046, pad=0.04)\n",
    "\n",
    "    # plot sampled spikes with colorbar\n",
    "    im = axs[1, 1].imshow(sampled_spikes[:, sampled_mask_idx_with_1], cmap=\"Greys\", alpha=1.0, aspect=\"auto\")\n",
    "    axs[1, 1].set_title(\"Sampled spikes\")\n",
    "    fig.colorbar(im, ax=axs[1, 1], orientation=\"vertical\", fraction=0.046, pad=0.04)\n",
    "\n",
    "    # print shapes for debugging\n",
    "    print(\n",
    "        real_rates[:, real_mask_idx_with_1].shape,\n",
    "        sampled_rates[:, sampled_mask_idx_with_1].shape,\n",
    "        real_spikes[:, real_mask_idx_with_1].shape,\n",
    "        sampled_spikes[:, sampled_mask_idx_with_1].shape,\n",
    "    )\n",
    "\n",
    "    # remove y-ticks from right plots for cleaner visualization\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        if i % 2 != 0:\n",
    "            ax.set_yticks([])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluation\n",
    "\n",
    "We first generate spikes from the trained model using the `sample_spikes_with_mask` function, and then compare with the ground truth spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_dict = sample_spikes_with_mask(\n",
    "    denoiser,\n",
    "    scheduler,\n",
    "    ae_model,\n",
    "    cfg,\n",
    "    batch_size=train_latent_dataloader.dataset.train_spike_masks[::6].shape[0],  # 1/6 of training data\n",
    "    lengths=[\n",
    "        l.sum() for l in train_latent_dataloader.dataset.train_spike_masks[::6, 0]\n",
    "    ],  # corresponding to 1/6 of training data\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training spikes and masks, using only 1/6 of the data for faster evaluation\n",
    "(\n",
    "    train_spikes,\n",
    "    train_masks,\n",
    ") = (\n",
    "    train_latent_dataloader.dataset.train_spikes[::6],  # 1/6 of training data\n",
    "    train_latent_dataloader.dataset.train_spike_masks[::6],  # 1/6 of training data\n",
    ")\n",
    "\n",
    "# trim spikes to only include timesteps where mask is 1\n",
    "train_spikes_trimmed = []\n",
    "\n",
    "for i in range(len(train_spikes)):\n",
    "    # get indices where mask is 1 for this sequence\n",
    "    nonzero_mask = train_masks[i, 0].nonzero().flatten()\n",
    "    spike = train_spikes[i]\n",
    "    # select only timesteps with mask=1\n",
    "    spike_ = spike[:, nonzero_mask]\n",
    "\n",
    "    train_spikes_trimmed.append(spike_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing trial length distribution\n",
    "fig = plt.figure(figsize=cm2inch(4, 1.5), dpi=300)\n",
    "\n",
    "# calculate the length of training spikes in seconds\n",
    "spikes_train_len = np.array([t.shape[-1] for t in train_spikes_trimmed])\n",
    "\n",
    "# plot histogram of trial lengths, converting to seconds\n",
    "_1, bins, _2 = plt.hist(spikes_train_len / 50, color=\"grey\", alpha=0.99, bins=40)\n",
    "plt.yticks([])\n",
    "plt.xticks([2, 6, 10])\n",
    "plt.gca().spines[\"left\"].set_visible(False)\n",
    "plt.xlabel(\"trial length (s)\")\n",
    "\n",
    "# print average trial length in seconds\n",
    "print(f\"{spikes_train_len.mean()/50}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get spikes and masks from diffusion model output\n",
    "sampled_spikes, sampled_masks = ret_dict[\"spikes\"], ret_dict[\"masks\"]\n",
    "\n",
    "# list to store trimmed spike sequences\n",
    "sampled_spikes_trimmed = []\n",
    "\n",
    "# iterate through each sequence\n",
    "for i in range(len(sampled_spikes)):\n",
    "    # get indices where mask is 1 for this sequence\n",
    "    nonzero_mask = sampled_masks[i, 0].nonzero().flatten()\n",
    "    # get spike sequence\n",
    "    spike = sampled_spikes[i]\n",
    "    # select only timesteps with mask=1\n",
    "    spike_ = spike[:, nonzero_mask]\n",
    "\n",
    "    # append trimmed sequence to list\n",
    "    sampled_spikes_trimmed.append(spike_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do the same thing for the autoencoder reconstruction\n",
    "\n",
    "rec_dict = reconstruct_spikes(ae_model, train_dataloader)\n",
    "\n",
    "rec_train_spikes = rec_dict[\"rec_spikes\"][::6]\n",
    "rec_train_spike_masks = rec_dict[\"signal_masks\"][::6]\n",
    "\n",
    "# list to store trimmed reconstructed spike sequences\n",
    "rec_train_spikes_trimmed = []\n",
    "\n",
    "# iterate through each sequence\n",
    "for i in range(len(rec_train_spikes)):\n",
    "    # get indices where mask is 1 for this sequence\n",
    "    nonzero_mask = rec_train_spikes[i, 0].nonzero().flatten()\n",
    "    # get spike sequence\n",
    "    spike = sampled_spikes[i]\n",
    "    # select only timesteps with mask=1\n",
    "    spike_ = spike[:, nonzero_mask]\n",
    "\n",
    "    # append trimmed sequence to list\n",
    "    rec_train_spikes_trimmed.append(spike_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all trimmed spike sequences along time dimension\n",
    "# this gives us one long sequence per neuron\n",
    "sampled_spikes_trimmed_cat = torch.cat(sampled_spikes_trimmed, dim=-1)  # sampled spikes from diffusion model\n",
    "train_spikes_trimmed_cat = torch.cat(train_spikes_trimmed, dim=-1)  # original training data spikes\n",
    "rec_train_spikes_trimmed_cat = torch.cat(rec_train_spikes_trimmed, dim=-1)  # reconstructed spikes from autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting correlation matrix, compute neuron-neuron correlation\n",
    "\n",
    "corrcoefs_train = np.corrcoef(train_spikes_trimmed_cat)\n",
    "corrcoefs_sampled = np.corrcoef(sampled_spikes_trimmed_cat)\n",
    "corrcoefs_rec = np.corrcoef(rec_train_spikes_trimmed_cat)\n",
    "np.fill_diagonal(corrcoefs_train, 0)\n",
    "np.fill_diagonal(corrcoefs_sampled, 0)\n",
    "np.fill_diagonal(corrcoefs_rec, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=cm2inch(9.8, 3), dpi=300)\n",
    "\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[2].axis(\"off\")\n",
    "\n",
    "# Get vmin/vmax from corrcoefs_train\n",
    "vmin = corrcoefs_train.min()\n",
    "vmax = corrcoefs_train.max()\n",
    "\n",
    "im = ax[0].imshow(\n",
    "    corrcoefs_train,\n",
    "    cmap=\"Reds\",\n",
    "    vmin=vmin,\n",
    "    vmax=vmax,\n",
    "    aspect=\"auto\",\n",
    ")\n",
    "cbar = plt.colorbar(\n",
    "    im,\n",
    "    ax=ax[0],\n",
    "    orientation=\"vertical\",\n",
    "    fraction=0.046,\n",
    "    pad=0.04,\n",
    ")\n",
    "\n",
    "# all colorbars have 0.0 and 0.5 tick labels\n",
    "cbar.set_ticks([0.0, 0.5])\n",
    "cbar.set_ticklabels([0.0, 0.5])\n",
    "im = ax[1].imshow(\n",
    "    corrcoefs_rec,\n",
    "    cmap=\"Reds\",\n",
    "    vmin=vmin,\n",
    "    vmax=vmax,\n",
    "    aspect=\"auto\",\n",
    ")\n",
    "cbar = plt.colorbar(\n",
    "    im,\n",
    "    ax=ax[1],\n",
    "    orientation=\"vertical\",\n",
    "    fraction=0.046,\n",
    "    pad=0.04,\n",
    ")\n",
    "\n",
    "# all colorbars have 0.0 and 0.5 tick labels\n",
    "cbar.set_ticks([0.0, 0.5])\n",
    "cbar.set_ticklabels([0.0, 0.5])\n",
    "im = ax[2].imshow(\n",
    "    corrcoefs_sampled,\n",
    "    cmap=\"Reds\",\n",
    "    vmin=vmin,\n",
    "    vmax=vmax,\n",
    "    aspect=\"auto\",\n",
    ")\n",
    "cbar = plt.colorbar(\n",
    "    im,\n",
    "    ax=ax[2],\n",
    "    orientation=\"vertical\",\n",
    "    fraction=0.046,\n",
    "    pad=0.04,\n",
    ")\n",
    "\n",
    "ax[0].set_title(\"gt neuron corr\")\n",
    "ax[1].set_title(\"ldns (ae) neuron corr\")\n",
    "ax[2].set_title(\"ldns (diff) neuron corr\")\n",
    "\n",
    "# all colorbars have 0.0 and 0.5 tick labels\n",
    "cbar.set_ticks([0.0, 0.5])\n",
    "cbar.set_ticklabels([0.0, 0.5])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\n",
    "    corrcoefs_sampled.min(),\n",
    "    corrcoefs_sampled.max(),\n",
    "    corrcoefs_train.min(),\n",
    "    corrcoefs_train.max(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=cm2inch(4, 4), dpi=300)\n",
    "\n",
    "# Scatter plot comparing ground truth, diffusion, and autoencoder correlations\n",
    "plt.scatter(\n",
    "    corrcoefs_train.flatten()[::1],\n",
    "    corrcoefs_sampled.flatten()[::1],\n",
    "    alpha=0.1,\n",
    "    s=1,\n",
    "    color=\"darkblue\",\n",
    "    rasterized=True,\n",
    "    label=\"ldns diffusion\",\n",
    ")\n",
    "plt.scatter(\n",
    "    corrcoefs_train.flatten()[::1],\n",
    "    corrcoefs_rec.flatten()[::1],\n",
    "    alpha=0.1,\n",
    "    s=1,\n",
    "    color=\"darkred\",\n",
    "    rasterized=True,\n",
    "    label=\"ldns ae\",\n",
    ")\n",
    "\n",
    "# Determine global min and max for axis scaling\n",
    "min_global = min(corrcoefs_train.min(), corrcoefs_sampled.min(), corrcoefs_rec.min())\n",
    "max_global = max(corrcoefs_train.max(), corrcoefs_sampled.max(), corrcoefs_rec.max())\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"gt\")\n",
    "plt.ylabel(\"ldns\")\n",
    "plt.title(\"neuron vs neuron correlation\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Add diagonal reference line\n",
    "x = np.linspace(min_global, max_global, 10)\n",
    "plt.plot(x, x, \"k--\", zorder=-10, alpha=0.99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for population spike count\n",
    "summed_spikes_train = np.concatenate([t.sum(0) for t in train_spikes_trimmed])\n",
    "summed_spikes_sampled = np.concatenate([t.sum(0) for t in sampled_spikes_trimmed])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for spike stats (isi), convert to spiketrain. This might be a bit slow.\n",
    "\n",
    "from ldns.utils.eval_utils import counts_to_spike_trains_ragged\n",
    "\n",
    "fps = 1000 / 20\n",
    "spike_trains_train_spiketrain = counts_to_spike_trains_ragged(\n",
    "    [t.permute(1, 0).numpy() for t in train_spikes_trimmed], fps=fps\n",
    ")\n",
    "spike_trains_sampled_spiketrain = counts_to_spike_trains_ragged(\n",
    "    [t.permute(1, 0).numpy() for t in sampled_spikes_trimmed], fps=fps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute spike stats\n",
    "from ldns.utils.eval_utils import compute_spike_stats_per_neuron\n",
    "from ldns.utils.plotting_utils import cm2inch\n",
    "\n",
    "spike_stats_gt = compute_spike_stats_per_neuron(\n",
    "    spike_trains_train_spiketrain,\n",
    "    n_samples=len(train_spikes_trimmed),\n",
    "    n_neurons=train_spikes_trimmed[0].shape[0],\n",
    "    mean_output=False,\n",
    ")\n",
    "spike_stats_sampled = compute_spike_stats_per_neuron(\n",
    "    spike_trains_sampled_spiketrain,\n",
    "    n_samples=len(sampled_spikes_trimmed),\n",
    "    n_neurons=sampled_spikes_trimmed[0].shape[0],\n",
    "    mean_output=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# create figure with 4 subplots (pop spike count, neuron-neuron corr, mean isi, std isi)\n",
    "fig, ax = plt.subplots(2, 2, figsize=cm2inch(9, 8))\n",
    "ax = ax.flatten()\n",
    "\n",
    "# clip spike counts to reasonable range\n",
    "gt_spikes = torch.tensor(summed_spikes_train).clip(0, 220)\n",
    "\n",
    "# compute kernel density estimates\n",
    "kde_gt = gaussian_kde(gt_spikes.flatten())\n",
    "kde_sampled = gaussian_kde(sampled_spikes_trimmed_cat.flatten())\n",
    "\n",
    "# evaluate densities on common grid\n",
    "x_grid = np.linspace(0, 220, 220 + 1)\n",
    "\n",
    "density_model = kde_sampled(x_grid)\n",
    "density_gt = kde_gt(x_grid)\n",
    "# normalize densities\n",
    "density_model /= density_model.sum()\n",
    "density_gt /= density_gt.sum()\n",
    "\n",
    "# plot population spike count distributions\n",
    "bins_psc = np.arange(-0.5, 220 - 0.5, 1)\n",
    "ax[0].hist(gt_spikes, bins=bins_psc, density=True, alpha=0.5, label=\"data\", color=\"grey\", rasterized=True)\n",
    "ax[0].hist(\n",
    "    sampled_spikes_trimmed_cat, bins=bins_psc, density=True, alpha=0.5, label=\"ldns\", color=\"darkred\", rasterized=True\n",
    ")\n",
    "ax[0].plot(x_grid, density_gt, \".-\", label=\"data kde\", color=\"black\", rasterized=False)\n",
    "ax[0].plot(x_grid, density_model, \".-\", label=\"ldns kde\", color=\"darkred\", rasterized=False)\n",
    "\n",
    "ax[0].set_xlim(40, 160)\n",
    "ax[0].set_yticks([])\n",
    "ax[0].spines[\"left\"].set_visible(False)\n",
    "ax[0].legend(fontsize=7)\n",
    "ax[0].set_xlabel(\"spike count\")\n",
    "\n",
    "# plot correlation structure comparison\n",
    "C_model = corrcoefs_sampled\n",
    "C_gt = corrcoefs_train\n",
    "np.fill_diagonal(C_model, 0)\n",
    "np.fill_diagonal(C_gt, 0)\n",
    "C_model = np.tril(C_model, k=-1)\n",
    "C_gt = np.tril(C_gt, k=-1)\n",
    "\n",
    "ax[1].plot(C_gt.flatten(), C_model.flatten(), \".\", alpha=0.3, color=\"darkred\", ms=2, rasterized=True)\n",
    "data_limits = [min(C_gt.min(), C_model.min()), max(C_gt.max(), C_model.max())]\n",
    "ax[1].plot([data_limits[0], data_limits[1]], [data_limits[0], data_limits[1]], \"--\", color=\"black\")\n",
    "ax[1].set_xlabel(\"gt\")\n",
    "ax[1].set_ylabel(\"ldns\")\n",
    "ax[1].set_aspect(\"equal\")\n",
    "data_limis_ax = [data_limits[0] - 0.15 * np.abs(data_limits[0]), data_limits[1] + 0.15 * np.abs(data_limits[1])]\n",
    "ax[1].set_xlim(data_limis_ax)\n",
    "ax[1].set_ylim(data_limis_ax)\n",
    "\n",
    "# plot mean ISI comparison\n",
    "ax[2].plot(\n",
    "    spike_stats_gt[\"mean_isi\"].flatten(),\n",
    "    spike_stats_sampled[\"mean_isi\"].flatten(),\n",
    "    \".\",\n",
    "    alpha=0.5,\n",
    "    color=\"darkred\",\n",
    "    ms=2,\n",
    "    rasterized=True,\n",
    ")\n",
    "data_limits = [\n",
    "    min(spike_stats_gt[\"mean_isi\"].flatten().min(), spike_stats_sampled[\"mean_isi\"].flatten().min()),\n",
    "    max(spike_stats_gt[\"mean_isi\"].flatten().max(), spike_stats_sampled[\"mean_isi\"].flatten().max()),\n",
    "]\n",
    "ax[2].plot([data_limits[0], data_limits[1]], [data_limits[0], data_limits[1]], \"--\", color=\"black\")\n",
    "ax[2].set_xlabel(\"gt mean isi\")\n",
    "ax[2].set_ylabel(\"ldns mean isi\")\n",
    "ax[2].set_aspect(\"equal\")\n",
    "data_limis_ax = [data_limits[0] - 0.15 * np.abs(data_limits[0]), data_limits[1] + 0.15 * np.abs(data_limits[1])]\n",
    "ax[2].set_xlim(data_limis_ax)\n",
    "ax[2].set_ylim(data_limis_ax)\n",
    "\n",
    "# plot ISI std comparison\n",
    "ax[3].plot(\n",
    "    spike_stats_gt[\"std_isi\"].flatten(),\n",
    "    spike_stats_sampled[\"std_isi\"].flatten(),\n",
    "    \".\",\n",
    "    alpha=0.5,\n",
    "    color=\"darkred\",\n",
    "    ms=2,\n",
    "    rasterized=True,\n",
    ")\n",
    "data_limits = [\n",
    "    min(spike_stats_gt[\"std_isi\"].flatten().min(), spike_stats_sampled[\"std_isi\"].flatten().min()),\n",
    "    max(spike_stats_gt[\"std_isi\"].flatten().max(), spike_stats_sampled[\"std_isi\"].flatten().max()),\n",
    "]\n",
    "ax[3].plot([data_limits[0], data_limits[1]], [data_limits[0], data_limits[1]], \"--\", color=\"black\")\n",
    "ax[3].set_xlabel(\"gt std isi\")\n",
    "ax[3].set_ylabel(\"ldns std isi\")\n",
    "ax[3].set_aspect(\"equal\")\n",
    "data_limis_ax = [data_limits[0] - 0.15 * np.abs(data_limits[0]), data_limits[1] + 0.15 * np.abs(data_limits[1])]\n",
    "ax[3].set_xlim(data_limis_ax)\n",
    "ax[3].set_ylim(data_limis_ax)\n",
    "ax[3].set_xticks([0.05, 0.15])\n",
    "ax[3].set_yticks([0.05, 0.15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure with two subplots for comparing correlation matrices\n",
    "fig, ax = plt.subplots(1, 2, figsize=cm2inch(6.5, 3), dpi=300)\n",
    "\n",
    "# plot correlation matrix for sampled data\n",
    "im = ax[0].imshow(corrcoefs_sampled, cmap=\"Reds\", vmin=-0.2, vmax=0.35, aspect=\"auto\")\n",
    "ax[0].axis(\"off\")\n",
    "cbar = plt.colorbar(im, ax=ax[0], orientation=\"vertical\", fraction=0.046, pad=0.04)\n",
    "ax[0].set_title(\"ldns neuron corr\")\n",
    "\n",
    "# plot correlation matrix for ground truth data\n",
    "im = ax[1].imshow(corrcoefs_train, cmap=\"Reds\", vmin=-0.2, vmax=0.35, aspect=\"auto\")\n",
    "cbar = plt.colorbar(im, ax=ax[1], orientation=\"vertical\", fraction=0.046, pad=0.04)\n",
    "ax[1].set_title(\"gt neuron corr\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "# adjust spacing between subplots\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure with two subplots for comparing correlation matrices\n",
    "fig, ax = plt.subplots(1, 2, figsize=cm2inch(6.5, 3), dpi=300)\n",
    "\n",
    "# plot correlation matrix for sampled data\n",
    "im = ax[0].imshow(corrcoefs_sampled, cmap=\"Reds\", vmin=-0.2, vmax=0.35, aspect=\"auto\")\n",
    "ax[0].axis(\"off\")\n",
    "cbar = plt.colorbar(im, ax=ax[0], orientation=\"vertical\", fraction=0.046, pad=0.04)\n",
    "ax[0].set_title(\"ldns neuron corr\")\n",
    "\n",
    "# plot correlation matrix for ground truth data\n",
    "im = ax[1].imshow(corrcoefs_train, cmap=\"Reds\", vmin=-0.2, vmax=0.35, aspect=\"auto\")\n",
    "cbar = plt.colorbar(im, ax=ax[1], orientation=\"vertical\", fraction=0.046, pad=0.04)\n",
    "ax[1].set_title(\"gt neuron corr\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "# adjust spacing between subplots\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot two examples of ground truth spike trains\n",
    "fig = plt.figure(figsize=cm2inch(6, 2), dpi=300)\n",
    "\n",
    "# plot first example with consistent vmin/vmax for comparison\n",
    "vmin, vmax = 0, 5\n",
    "im = plt.imshow(train_spikes[5, :], cmap=\"Greys\", alpha=1.0, aspect=\"auto\", vmin=vmin, vmax=vmax)\n",
    "\n",
    "# clean up axes\n",
    "plt.xticks([])\n",
    "plt.gca().spines[\"bottom\"].set_visible(False)\n",
    "plt.yticks([])\n",
    "plt.ylabel(\"neurons\")\n",
    "plt.show()\n",
    "\n",
    "# plot second example\n",
    "fig = plt.figure(figsize=cm2inch(6, 2), dpi=300)\n",
    "\n",
    "im = plt.imshow(train_spikes[0, :], cmap=\"Greys\", alpha=1.0, aspect=\"auto\", vmin=vmin, vmax=vmax)\n",
    "\n",
    "# add time axis labels\n",
    "plt.xticks([0, 500])\n",
    "plt.gca().set_xticklabels([0, 10])\n",
    "plt.xlabel(\"time (s)\")\n",
    "\n",
    "# clean up y axis\n",
    "plt.yticks([])\n",
    "plt.ylabel(\"neurons\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot three examples of sampled spike trains\n",
    "fig = plt.figure(figsize=cm2inch(6, 2), dpi=300)\n",
    "vmax = 4\n",
    "\n",
    "# get indices of non-zero mask values for first example\n",
    "sampled_mask_idx_with_11 = sampled_masks[248, 0].nonzero().flatten()\n",
    "\n",
    "# pad sampled spikes with zeros to match dimensions\n",
    "sampled_spikes_padded2 = torch.cat(\n",
    "    (sampled_spikes[248, :, sampled_mask_idx_with_11], torch.zeros(128, 512 - len(sampled_mask_idx_with_11))), dim=-1\n",
    ")\n",
    "print(sampled_spikes_padded2.shape)\n",
    "\n",
    "# plot first example\n",
    "plt.imshow(sampled_spikes_padded2, cmap=\"Greys\", alpha=1.0, aspect=\"auto\", vmin=0, vmax=vmax)\n",
    "plt.xticks([])\n",
    "plt.gca().spines[\"bottom\"].set_visible(False)\n",
    "plt.yticks([])\n",
    "plt.ylabel(\"neurons\")\n",
    "plt.show()\n",
    "\n",
    "# plot second example\n",
    "fig = plt.figure(figsize=cm2inch(6, 2), dpi=300)\n",
    "sampled_mask_idx_with_12 = sampled_masks[16, 0].nonzero().flatten()\n",
    "sampled_spikes_padded2 = torch.cat(\n",
    "    (sampled_spikes[16, :, sampled_mask_idx_with_12], torch.zeros(128, 512 - len(sampled_mask_idx_with_12))), dim=-1\n",
    ")\n",
    "plt.imshow(sampled_spikes_padded2, cmap=\"Greys\", alpha=1.0, aspect=\"auto\", vmin=0, vmax=vmax)\n",
    "plt.xticks([])\n",
    "plt.gca().spines[\"bottom\"].set_visible(False)\n",
    "plt.yticks([])\n",
    "plt.ylabel(\"neurons\")\n",
    "plt.show()\n",
    "\n",
    "# plot third example with time axis\n",
    "fig = plt.figure(figsize=cm2inch(6, 2), dpi=300)\n",
    "sampled_mask_idx_with_13 = sampled_masks[89, 0].nonzero().flatten()\n",
    "sampled_spikes_padded2 = torch.cat(\n",
    "    (sampled_spikes[89, :, sampled_mask_idx_with_13], torch.zeros(128, 512 - len(sampled_mask_idx_with_13))), dim=-1\n",
    ")\n",
    "plt.imshow(sampled_spikes_padded2, cmap=\"Greys\", alpha=1.0, aspect=\"auto\", vmin=0, vmax=vmax)\n",
    "plt.xticks([0, 500])\n",
    "plt.gca().set_xticklabels([0, 10])\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.yticks([])\n",
    "plt.ylabel(\"neurons\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA of smoothed spikes, data vs sampled\n",
    "\n",
    "We will train PCA on one subset of smoothed spikes and transform the other subset to compare the latent spaces\n",
    "\n",
    "We will also compare this to the latent space of sampled spikes from the diffusion model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages for PCA and signal processing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import signal\n",
    "\n",
    "# define parameters for smoothing window\n",
    "time_per_bin = 0.02  # 20 ms bin size for spike data\n",
    "win_len = 8  # window length in number of bins (8 * 20ms = 160ms total)\n",
    "win_std = 0.06  # standard deviation of gaussian window in seconds\n",
    "num_bins_std = int(win_std / time_per_bin)  # convert std from seconds to bins\n",
    "\n",
    "# create gaussian smoothing window\n",
    "smo_window = signal.windows.gaussian(int(win_len * num_bins_std), num_bins_std)\n",
    "smo_window /= smo_window.sum()  # normalize window to sum to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ground truth spike data\n",
    "ground_truth_spikes = train_spikes_trimmed\n",
    "\n",
    "# convert to numpy arrays for processing\n",
    "ground_truth_np = [t.numpy() for t in ground_truth_spikes]\n",
    "print(ground_truth_np[0].shape, ground_truth_np[1].shape, len(ground_truth_np))\n",
    "\n",
    "# smooth each spike train with gaussian window\n",
    "smoothed_spikes = []\n",
    "for spike_train in ground_truth_np:\n",
    "    # apply convolution along time axis for each neuron\n",
    "    smoothed_spikes.append(\n",
    "        np.apply_along_axis(lambda m: np.convolve(m, smo_window, mode=\"same\"), axis=1, arr=spike_train)\n",
    "    )\n",
    "\n",
    "print(smoothed_spikes[0].shape, smoothed_spikes[1].shape, len(smoothed_spikes))\n",
    "\n",
    "# randomly split smoothed data into two halves for cross-validation\n",
    "split_indices = np.random.choice(len(smoothed_spikes), len(smoothed_spikes) // 2, replace=False)\n",
    "smoothed_train = [smoothed_spikes[i] for i in split_indices]\n",
    "# get complementary indices for test set\n",
    "smoothed_test = [smoothed_spikes[i] for i in range(len(smoothed_spikes)) if i not in split_indices]\n",
    "\n",
    "print(len(smoothed_train), len(smoothed_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train PCA on one subset, transform the other\n",
    "smoothed_train_reshaped = [rearrange(t, \"c l -> l c\") for t in smoothed_train]\n",
    "smoothed_train_concat = np.concatenate(smoothed_train_reshaped, axis=0)\n",
    "print(smoothed_train_concat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of test samples\n",
    "num_smoothed_test = len(smoothed_test)\n",
    "\n",
    "# get length of each test sequence\n",
    "smoothed_test_lengths = [t.shape[-1] for t in smoothed_test]\n",
    "\n",
    "# reshape test data from (channels, length) to (length, channels)\n",
    "smoothed_test_reshaped = [rearrange(t, \"c l -> l c\") for t in smoothed_test]\n",
    "\n",
    "# concatenate all test sequences into single array\n",
    "smoothed_test_concat = np.concatenate(smoothed_test_reshaped, axis=0)\n",
    "print(smoothed_test_concat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize training data by removing mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "train_data_standardized = scaler.fit_transform(smoothed_train_concat)\n",
    "\n",
    "# fit PCA to reduce dimensionality to 4 components\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(train_data_standardized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import pack, unpack\n",
    "\n",
    "# transform ground truth data using fitted PCA and scaler\n",
    "gt_transformed = pca.transform(scaler.transform(smoothed_test_concat))\n",
    "\n",
    "# split transformed data back into original sequence lengths\n",
    "gt_transformed_sequences = []\n",
    "for i, length in enumerate(np.cumsum(smoothed_test_lengths)):\n",
    "    seq_start = length - smoothed_test_lengths[i]\n",
    "    gt_transformed_sequences.append(gt_transformed[seq_start:length])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform sampled spikes data similar to ground truth data\n",
    "# convert pytorch tensors to numpy arrays\n",
    "sampled_spikes_np = [t.numpy() for t in sampled_spikes_trimmed]\n",
    "\n",
    "# smooth each sequence using convolution with smoothing window\n",
    "smoothed_sampled = []\n",
    "for spikes in sampled_spikes_np:\n",
    "    smoothed_sampled.append(np.apply_along_axis(lambda m: np.convolve(m, smo_window, mode=\"same\"), axis=1, arr=spikes))\n",
    "\n",
    "# get length of each smoothed sequence\n",
    "smoothed_sampled_lengths = [t.shape[-1] for t in smoothed_sampled]\n",
    "\n",
    "# reshape from (channels, length) to (length, channels) like ground truth\n",
    "smoothed_reshaped = [rearrange(t, \"c l -> l c\") for t in smoothed_sampled]\n",
    "\n",
    "# concatenate all sequences into single array\n",
    "smoothed_sampled_concat = np.concatenate(smoothed_reshaped, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize smoothed sampled data using fitted scaler\n",
    "sampled_spikes_trimmed_smoothed_standardized = scaler.transform(smoothed_sampled_concat)\n",
    "\n",
    "# transform standardized data using fitted PCA\n",
    "sampled_spikes_trimmed_smoothed_transformed = pca.transform(sampled_spikes_trimmed_smoothed_standardized)\n",
    "\n",
    "# split transformed data back into original sequence lengths\n",
    "sampled_transformed = []\n",
    "for i, length in enumerate(np.cumsum(smoothed_sampled_lengths)):\n",
    "    sampled_transformed.append(\n",
    "        sampled_spikes_trimmed_smoothed_transformed[length - smoothed_sampled_lengths[i] : length]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure with 2x2 subplots for comparing ground truth vs sampled trajectories in PC space\n",
    "fig, ax = plt.subplots(2, 2, figsize=cm2inch(6, 10), dpi=300, sharey=True)\n",
    "\n",
    "# plot every 50th trajectory from ground truth and sampled data\n",
    "for i in range(len(gt_transformed_sequences) // 50):\n",
    "    # plot ground truth trajectories in grey\n",
    "    ax[0, 0].plot(gt_transformed_sequences[i * 50][:, 0], alpha=0.2, color=\"grey\")  # pc1\n",
    "    ax[1, 0].plot(gt_transformed_sequences[i * 50][:, 1], alpha=0.2, color=\"grey\")  # pc2\n",
    "\n",
    "    # plot sampled trajectories in red\n",
    "    ax[0, 1].plot(sampled_transformed[i * 50][:, 0], alpha=0.2, color=\"#A44A3F\")  # pc1\n",
    "    ax[1, 1].plot(sampled_transformed[i * 50][:, 1], alpha=0.2, color=\"#A44A3F\")  # pc2\n",
    "\n",
    "# add y-axis labels for principal components\n",
    "ax[0, 0].set_ylabel(\"PC1\")\n",
    "ax[1, 0].set_ylabel(\"PC2\")\n",
    "\n",
    "# add x-axis labels for time\n",
    "ax[1, 0].set_xlabel(\"time (s)\")\n",
    "ax[1, 1].set_xlabel(\"time (s)\")\n",
    "\n",
    "# set consistent x-axis limits and ticks for all subplots\n",
    "for ax_ in ax.flatten():\n",
    "    ax_.set_xlim(-10, 500)\n",
    "    ax_.set_xticks([0, 250, 500])\n",
    "\n",
    "# remove x-tick labels from top row\n",
    "for ax_ in ax[0]:\n",
    "    ax_.set_xticklabels([])\n",
    "\n",
    "# convert x-ticks from samples to seconds for bottom row\n",
    "for ax_ in ax[1]:\n",
    "    ax_.set_xticklabels([0 / 50, 250 / 50, 500 / 50])\n",
    "\n",
    "# add titles to distinguish ground truth from sampled data\n",
    "ax[0, 0].set_title(\"gt\")\n",
    "ax[0, 1].set_title(\"ldns\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeseries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
